{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd0c05f7f5b7449b362e25f922c76429131874e0839c45bc04f41f76db7906a2c56",
   "display_name": "Python 3.8.2 64-bit ('coursera_tensorflow_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-05-26 19:29:28--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.195.48, 142.250.195.112, 142.250.196.80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.195.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68970 (67K) [text/plain]\n",
      "Saving to: ‘./Data/irish-lyrics-eof.txt’\n",
      "\n",
      "./Data/irish-lyrics 100%[===================>]  67.35K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2021-05-26 19:29:29 (790 KB/s) - ‘./Data/irish-lyrics-eof.txt’ saved [68970/68970]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt -O ./Data/irish-lyrics-eof.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_path = \"./Data/irish-lyrics-eof.txt\"\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of lines in the poem :  1692\n"
     ]
    }
   ],
   "source": [
    "with open(poem_path, 'r') as poem_file:\n",
    "    poem_data = poem_file.readlines()\n",
    "\n",
    "print(\"No of lines in the poem : \", len(poem_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(poem_data)\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data size :  12038\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "max_length = 0\n",
    "\n",
    "for line in poem_data:\n",
    "    sequence = tokenizer.texts_to_sequences([line])[0]\n",
    "    if len(sequence) > max_length:\n",
    "        max_length = len(sequence)\n",
    "    for seq_index in range(1, len(sequence)):\n",
    "        training_data.append(sequence[:seq_index+1])\n",
    "print(\"Train data size : \", len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[51, 12], [51, 12, 96], [51, 12, 96, 1223], [51, 12, 96, 1223, 48], [51, 12, 96, 1223, 48, 2]]\n[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    51   12]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0   51\n    12   96]\n [   0    0    0    0    0    0    0    0    0    0    0    0   51   12\n    96 1223]\n [   0    0    0    0    0    0    0    0    0    0    0   51   12   96\n  1223   48]\n [   0    0    0    0    0    0    0    0    0    0   51   12   96 1223\n    48    2]]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[:5])\n",
    "padded_train_data = pad_sequences(training_data, maxlen=max_length, padding=\"pre\")\n",
    "print(padded_train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 51]\n12\n"
     ]
    }
   ],
   "source": [
    "train_sentence, train_label = padded_train_data[:, :-1], padded_train_data[:, -1]\n",
    "\n",
    "print(train_sentence[0])\n",
    "print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_hot_encoded = tf.one_hot(train_label, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_16 (Embedding)     (None, 15, 100)           270000    \n_________________________________________________________________\nbidirectional_29 (Bidirectio (None, 300)               301200    \n_________________________________________________________________\ndense_16 (Dense)             (None, 2689)              809389    \n=================================================================\nTotal params: 1,380,589\nTrainable params: 1,380,589\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length-1),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.lay ers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dense(train_label_hot_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "377/377 [==============================] - 18s 40ms/step - loss: 6.9767 - accuracy: 0.0626\n",
      "Epoch 2/100\n",
      "377/377 [==============================] - 26s 70ms/step - loss: 6.2507 - accuracy: 0.0702\n",
      "Epoch 3/100\n",
      "377/377 [==============================] - 32s 86ms/step - loss: 5.9897 - accuracy: 0.0805\n",
      "Epoch 4/100\n",
      "377/377 [==============================] - 43s 115ms/step - loss: 5.6888 - accuracy: 0.0999\n",
      "Epoch 5/100\n",
      "377/377 [==============================] - 33s 88ms/step - loss: 5.3265 - accuracy: 0.1117\n",
      "Epoch 6/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 4.9668 - accuracy: 0.1334\n",
      "Epoch 7/100\n",
      "377/377 [==============================] - 32s 86ms/step - loss: 4.6114 - accuracy: 0.1585\n",
      "Epoch 8/100\n",
      "377/377 [==============================] - 33s 89ms/step - loss: 4.2229 - accuracy: 0.1872\n",
      "Epoch 9/100\n",
      "377/377 [==============================] - 31s 82ms/step - loss: 3.9057 - accuracy: 0.2274\n",
      "Epoch 10/100\n",
      "377/377 [==============================] - 32s 85ms/step - loss: 3.5156 - accuracy: 0.2804\n",
      "Epoch 11/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 3.2059 - accuracy: 0.3378\n",
      "Epoch 12/100\n",
      "377/377 [==============================] - 32s 85ms/step - loss: 2.8600 - accuracy: 0.4032\n",
      "Epoch 13/100\n",
      "377/377 [==============================] - 34s 90ms/step - loss: 2.6046 - accuracy: 0.4509\n",
      "Epoch 14/100\n",
      "377/377 [==============================] - 34s 91ms/step - loss: 2.3120 - accuracy: 0.5124\n",
      "Epoch 15/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 2.1315 - accuracy: 0.5529\n",
      "Epoch 16/100\n",
      "377/377 [==============================] - 32s 85ms/step - loss: 1.9035 - accuracy: 0.5999\n",
      "Epoch 17/100\n",
      "377/377 [==============================] - 35s 92ms/step - loss: 1.7271 - accuracy: 0.6360\n",
      "Epoch 18/100\n",
      "377/377 [==============================] - 30s 80ms/step - loss: 1.5783 - accuracy: 0.6623\n",
      "Epoch 19/100\n",
      "377/377 [==============================] - 31s 83ms/step - loss: 1.4237 - accuracy: 0.6997\n",
      "Epoch 20/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 1.3060 - accuracy: 0.7308\n",
      "Epoch 21/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 1.2033 - accuracy: 0.7486\n",
      "Epoch 22/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 1.0873 - accuracy: 0.7754\n",
      "Epoch 23/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 1.0083 - accuracy: 0.7913\n",
      "Epoch 24/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 0.9620 - accuracy: 0.7975\n",
      "Epoch 25/100\n",
      "377/377 [==============================] - 35s 92ms/step - loss: 0.9026 - accuracy: 0.8089\n",
      "Epoch 26/100\n",
      "377/377 [==============================] - 33s 88ms/step - loss: 0.8126 - accuracy: 0.8225\n",
      "Epoch 27/100\n",
      "377/377 [==============================] - 30s 80ms/step - loss: 0.7845 - accuracy: 0.8288\n",
      "Epoch 28/100\n",
      "377/377 [==============================] - 30s 79ms/step - loss: 0.7384 - accuracy: 0.8342\n",
      "Epoch 29/100\n",
      "377/377 [==============================] - 35s 92ms/step - loss: 0.6999 - accuracy: 0.8400\n",
      "Epoch 30/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 0.6634 - accuracy: 0.8462\n",
      "Epoch 31/100\n",
      "377/377 [==============================] - 43s 115ms/step - loss: 0.6390 - accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 0.6314 - accuracy: 0.8472\n",
      "Epoch 33/100\n",
      "377/377 [==============================] - 34s 90ms/step - loss: 0.6156 - accuracy: 0.8477\n",
      "Epoch 34/100\n",
      "377/377 [==============================] - 31s 83ms/step - loss: 0.6200 - accuracy: 0.8413\n",
      "Epoch 35/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.5754 - accuracy: 0.8563\n",
      "Epoch 36/100\n",
      "377/377 [==============================] - 42s 111ms/step - loss: 0.5872 - accuracy: 0.8491\n",
      "Epoch 37/100\n",
      "377/377 [==============================] - 36s 96ms/step - loss: 0.5560 - accuracy: 0.8556\n",
      "Epoch 38/100\n",
      "377/377 [==============================] - 34s 91ms/step - loss: 0.5515 - accuracy: 0.8574\n",
      "Epoch 39/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.5353 - accuracy: 0.8574\n",
      "Epoch 40/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 0.5466 - accuracy: 0.8564\n",
      "Epoch 41/100\n",
      "377/377 [==============================] - 36s 94ms/step - loss: 0.5366 - accuracy: 0.8553\n",
      "Epoch 42/100\n",
      "377/377 [==============================] - 38s 102ms/step - loss: 0.5119 - accuracy: 0.8614\n",
      "Epoch 43/100\n",
      "377/377 [==============================] - 33s 88ms/step - loss: 0.5125 - accuracy: 0.8581\n",
      "Epoch 44/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 0.5328 - accuracy: 0.8546\n",
      "Epoch 45/100\n",
      "377/377 [==============================] - 35s 93ms/step - loss: 0.4988 - accuracy: 0.8593\n",
      "Epoch 46/100\n",
      "377/377 [==============================] - 34s 91ms/step - loss: 0.5304 - accuracy: 0.8534\n",
      "Epoch 47/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 0.5013 - accuracy: 0.8549\n",
      "Epoch 48/100\n",
      "377/377 [==============================] - 36s 96ms/step - loss: 0.5156 - accuracy: 0.8544\n",
      "Epoch 49/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 0.5047 - accuracy: 0.8577\n",
      "Epoch 50/100\n",
      "377/377 [==============================] - 42s 111ms/step - loss: 0.4907 - accuracy: 0.8615\n",
      "Epoch 51/100\n",
      "377/377 [==============================] - 40s 107ms/step - loss: 0.4777 - accuracy: 0.8623\n",
      "Epoch 52/100\n",
      "377/377 [==============================] - 43s 115ms/step - loss: 0.4876 - accuracy: 0.8627\n",
      "Epoch 53/100\n",
      "377/377 [==============================] - 34s 91ms/step - loss: 0.4795 - accuracy: 0.8602\n",
      "Epoch 54/100\n",
      "377/377 [==============================] - 34s 91ms/step - loss: 0.4935 - accuracy: 0.8561\n",
      "Epoch 55/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.4948 - accuracy: 0.8553\n",
      "Epoch 56/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.5079 - accuracy: 0.8534\n",
      "Epoch 57/100\n",
      "377/377 [==============================] - 27s 72ms/step - loss: 0.4701 - accuracy: 0.8645\n",
      "Epoch 58/100\n",
      "377/377 [==============================] - 27s 72ms/step - loss: 0.4817 - accuracy: 0.8619\n",
      "Epoch 59/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.4822 - accuracy: 0.8613\n",
      "Epoch 60/100\n",
      "377/377 [==============================] - 31s 83ms/step - loss: 0.4913 - accuracy: 0.8549\n",
      "Epoch 61/100\n",
      "377/377 [==============================] - 32s 84ms/step - loss: 0.4672 - accuracy: 0.8659\n",
      "Epoch 62/100\n",
      "377/377 [==============================] - 29s 77ms/step - loss: 0.4607 - accuracy: 0.8660\n",
      "Epoch 63/100\n",
      "377/377 [==============================] - 24s 64ms/step - loss: 0.4719 - accuracy: 0.8620\n",
      "Epoch 64/100\n",
      "377/377 [==============================] - 26s 69ms/step - loss: 0.4720 - accuracy: 0.8578\n",
      "Epoch 65/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 0.4636 - accuracy: 0.8641\n",
      "Epoch 66/100\n",
      "377/377 [==============================] - 32s 84ms/step - loss: 0.4762 - accuracy: 0.8575\n",
      "Epoch 67/100\n",
      "377/377 [==============================] - 34s 90ms/step - loss: 0.4648 - accuracy: 0.8569\n",
      "Epoch 68/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.4643 - accuracy: 0.8602\n",
      "Epoch 69/100\n",
      "377/377 [==============================] - 32s 84ms/step - loss: 0.4702 - accuracy: 0.8567\n",
      "Epoch 70/100\n",
      "377/377 [==============================] - 32s 86ms/step - loss: 0.4522 - accuracy: 0.8632\n",
      "Epoch 71/100\n",
      "377/377 [==============================] - 35s 92ms/step - loss: 0.4689 - accuracy: 0.8547\n",
      "Epoch 72/100\n",
      "377/377 [==============================] - 33s 87ms/step - loss: 0.4560 - accuracy: 0.8632\n",
      "Epoch 73/100\n",
      "377/377 [==============================] - 32s 85ms/step - loss: 0.4731 - accuracy: 0.8609\n",
      "Epoch 74/100\n",
      "377/377 [==============================] - 29s 78ms/step - loss: 0.4427 - accuracy: 0.8639\n",
      "Epoch 75/100\n",
      "377/377 [==============================] - 27s 73ms/step - loss: 0.4566 - accuracy: 0.8644\n",
      "Epoch 76/100\n",
      "377/377 [==============================] - 23s 62ms/step - loss: 0.4685 - accuracy: 0.8562\n",
      "Epoch 77/100\n",
      "377/377 [==============================] - 27s 71ms/step - loss: 0.4657 - accuracy: 0.8565\n",
      "Epoch 78/100\n",
      "377/377 [==============================] - 31s 83ms/step - loss: 0.4415 - accuracy: 0.8607\n",
      "Epoch 79/100\n",
      "377/377 [==============================] - 30s 78ms/step - loss: 0.4684 - accuracy: 0.8577\n",
      "Epoch 80/100\n",
      "377/377 [==============================] - 34s 90ms/step - loss: 0.4598 - accuracy: 0.8587\n",
      "Epoch 81/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 0.4602 - accuracy: 0.8601\n",
      "Epoch 82/100\n",
      "377/377 [==============================] - 27s 71ms/step - loss: 0.4511 - accuracy: 0.8598\n",
      "Epoch 83/100\n",
      "377/377 [==============================] - 31s 81ms/step - loss: 0.4396 - accuracy: 0.8627\n",
      "Epoch 84/100\n",
      "377/377 [==============================] - 34s 90ms/step - loss: 0.4548 - accuracy: 0.8636\n",
      "Epoch 85/100\n",
      "377/377 [==============================] - 30s 80ms/step - loss: 0.4636 - accuracy: 0.8601\n",
      "Epoch 86/100\n",
      "377/377 [==============================] - 28s 75ms/step - loss: 0.4372 - accuracy: 0.8643\n",
      "Epoch 87/100\n",
      "377/377 [==============================] - 25s 67ms/step - loss: 0.4467 - accuracy: 0.8623\n",
      "Epoch 88/100\n",
      "377/377 [==============================] - 26s 70ms/step - loss: 0.4493 - accuracy: 0.8623\n",
      "Epoch 89/100\n",
      "377/377 [==============================] - 26s 68ms/step - loss: 0.4634 - accuracy: 0.8570\n",
      "Epoch 90/100\n",
      "377/377 [==============================] - 32s 85ms/step - loss: 0.4519 - accuracy: 0.8566\n",
      "Epoch 91/100\n",
      "377/377 [==============================] - 31s 81ms/step - loss: 0.4482 - accuracy: 0.8559\n",
      "Epoch 92/100\n",
      "377/377 [==============================] - 30s 80ms/step - loss: 0.4663 - accuracy: 0.8560\n",
      "Epoch 93/100\n",
      "377/377 [==============================] - 27s 71ms/step - loss: 0.4596 - accuracy: 0.8596\n",
      "Epoch 94/100\n",
      "377/377 [==============================] - 28s 74ms/step - loss: 0.4489 - accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "377/377 [==============================] - 30s 79ms/step - loss: 0.4568 - accuracy: 0.8594\n",
      "Epoch 96/100\n",
      "377/377 [==============================] - 35s 94ms/step - loss: 0.4556 - accuracy: 0.8579\n",
      "Epoch 97/100\n",
      "377/377 [==============================] - 34s 89ms/step - loss: 0.4514 - accuracy: 0.8589\n",
      "Epoch 98/100\n",
      "377/377 [==============================] - 33s 88ms/step - loss: 0.4484 - accuracy: 0.8621\n",
      "Epoch 99/100\n",
      "377/377 [==============================] - 28s 75ms/step - loss: 0.4507 - accuracy: 0.8606\n",
      "Epoch 100/100\n",
      "377/377 [==============================] - 29s 76ms/step - loss: 0.4612 - accuracy: 0.8531\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sentence, train_label_hot_encoded, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output : \nI've got a bad feeling about this dead more more shook to town to the coat almost doubts even green and the young ones britches all green were the wind light and play on the rocky farmers daughter and the approaching tree to the town of help the wind maiden more more they might see but more more three dancing low be wear tuam fingers him quite hosannahs to low on beauty you were low more slower of dream inside gold of gold of gold and gold and deep shall only this beauty fly with your say visions for a tired sinking almost nature carrigfergus almost nature\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I've got a bad feeling about this\"\n",
    "next_word = 100\n",
    "\n",
    "for _ in range(next_word):\n",
    "    sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    padded_seq = pad_sequences([sequence], maxlen=max_length-1, padding='pre')\n",
    "    predicted_class = model.predict_classes(padded_seq)[0]\n",
    "    seed_text += \" \" + reverse_word_index[predicted_class]\n",
    "print(\"Output : \")\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}